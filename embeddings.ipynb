{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer-based Natural Language Processing\n",
    "## Introduction to PyTorch & Deep Learning in NLP \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/texttechnologylab/SoSe22-M-PNLR-PR-TbNLP/blob/main/embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aqcuiring Some Data\n",
    "\n",
    "- Use the code below to accquire some sentence-segmented data.\n",
    "    - Note: You may use also any other corpus available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowload a small dataset of sentences from the English Wikipedia from the \"Wortschatz\" project of the University Leipzig\n",
    "# - D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.\n",
    "#   In: Proceedings of the 8th International Language Resources and Evaluation (LREC'12), 2012\n",
    "!mkdir data\n",
    "!curl http://pcai056.informatik.uni-leipzig.de/downloads/corpora/eng_wikipedia_2016_10K.tar.gz -o data/eng_wikipedia_2016_10K.tar.gz\n",
    "!tar -xf data/eng_wikipedia_2016_10K.tar.gz -C data/\n",
    "\n",
    "plain_text_file = \"data/eng_wikipedia_2016_10K/eng_wikipedia_2016_10K-sentences.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with `ðŸ¤— datasets`\n",
    "\n",
    "1. Familiarize yourself with the `ðŸ¤— datasets` package and it's API.\n",
    "2. Load the plain text corpus that was downloaded using the code above.\n",
    "3. (Pre-)Process the data:\n",
    "    - Remove the line-number preceeding each sentence.\n",
    "    - Split the sentences into words/tokens.\n",
    "\n",
    "\n",
    "#### Resources\n",
    "\n",
    "- [`ðŸ¤— datasets` Documentation](https://huggingface.co/docs/datasets/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ...  # TODO\n",
    "\n",
    "corpus = ...  # TODO: Load the plain text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(*args, **kwargs):  # TODO\n",
    "    \"\"\"Split the sentence strings into lists of tokens and apply other transforms as necessary.\"\"\"\n",
    "    pass\n",
    "\n",
    "dataset = ...  # TODO: Process the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP in PyTorch &ndash; Basic Word2Vec\n",
    "\n",
    "Familizarize yourself with the Word2Vec model introduced in Mikolov et al. (2013).\n",
    "\n",
    "1. Implement the CBOW model (without hierarchical softmax or negative sampling improvements).\n",
    "2. Train the model on the small dataset from above.\n",
    "3. Evaluate your results.\n",
    "    - Note: The small dataset provided above is only sufficient for development, not to actually train a model that should give reasonable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- \"Efficient Estimation of Word Representations in Vector Space\", Mikolov et al., 2013, [arxiv.org/abs/1301.3781](http://arxiv.org/abs/1301.3781)\n",
    "\n",
    "![Mikolov et al. (2013)](images/cbow.png)\n",
    "\n",
    "#### Hints\n",
    "- Use the `torch.nn.Embedding` module to create your vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):  # TODO\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args, **kwargs):  # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(...)  # TODO: Model initialization\n",
    "optimizer = ...  # TODO: Optimizer intialization\n",
    "\n",
    "for epoch in ...:\n",
    "    model.train()\n",
    "    pass  # TODO: Training code goes here\n",
    "\n",
    "model.eval()\n",
    "pass  # TODO: Evaluation code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with `ðŸ¤— tokenizers`\n",
    "\n",
    "1. Implement a tokenization approach using the `ðŸ¤— tokenizers` library.\n",
    "    - There are [multiple different models](https://huggingface.co/docs/tokenizers/python/latest/components.html#models) of tokenizers available. Which one do you choose for the task at hand?\n",
    "2. Tokenize your dataset using the new tokenizer and rerun your experiment from above.\n",
    "3. Evaluate the results and compare them with the results from above.\n",
    "\n",
    "#### Resources\n",
    "\n",
    "- [`ðŸ¤— tokenizers` Documentation](https://huggingface.co/docs/tokenizers/python/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "\n",
    "def process_with_tokenizers(*args, **kwargs):  # TODO\n",
    "    pass\n",
    "\n",
    "dataset = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tk = CBOW(...)  # TODO: Model initialization\n",
    "optimizer = ...  # TODO: Optimizer intialization\n",
    "\n",
    "for epoch in ...:\n",
    "    model_tk.train()\n",
    "    pass  # TODO: Training code goes here\n",
    "\n",
    "model_tk.eval()\n",
    "pass  # TODO: Evaluation code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Wang2Vec\n",
    "\n",
    "1. Implement the CWINDOW model from Ling et. al (2015).\n",
    "2. Train the model on the same dataset as before.\n",
    "3. Evaluate and compare your results.\n",
    "\n",
    "#### Refernces\n",
    "- \"Two/Too Simple Adaptations of Word2Vec for Syntax Problems\", Ling et al., 2015, [aclanthology.org/N15-1142/](https://aclanthology.org/N15-1142/)\n",
    "\n",
    "\n",
    "![Ling et al. (2015)](images/cwindow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWINDOW(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):  # TODO\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args, **kwargs):  # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cwindow = CWINDOW(...)  # TODO: Model initialization\n",
    "optimizer = ...  # TODO: Optimizer intialization\n",
    "\n",
    "for epoch in ...:\n",
    "    model_cwindow.train()\n",
    "    pass  # TODO: Training code goes here\n",
    "\n",
    "model_cwindow.eval()\n",
    "pass  # TODO: Evaluation code goes here"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b267c91f0e969427353ee359f0d79efaf7083d633af26cd8d79a96b1a90613d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 ('py310-pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
